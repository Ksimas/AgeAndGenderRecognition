{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.layers import Reshape, MaxPooling2D\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\Envi36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\Envi36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#1a\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)))\n",
    "#1\n",
    "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "#2\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "#3\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "#4\n",
    "model.add(Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "#5\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 146, 146, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 144, 144, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 72, 72, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 70, 70, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 33, 33, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,499,297\n",
      "Trainable params: 3,499,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optymalizator\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer=optimizers.RMSprop(lr=1e-4), \n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19280 images belonging to 2 classes.\n",
      "Found 2410 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\Envi36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/140\n",
      "600/600 [==============================] - 344s 573ms/step - loss: 0.6686 - acc: 0.5863 - val_loss: 0.5960 - val_acc: 0.6869\n",
      "Epoch 2/140\n",
      "600/600 [==============================] - 355s 592ms/step - loss: 0.6232 - acc: 0.6505 - val_loss: 0.5526 - val_acc: 0.7148\n",
      "Epoch 3/140\n",
      "600/600 [==============================] - 363s 605ms/step - loss: 0.5912 - acc: 0.6777 - val_loss: 0.5357 - val_acc: 0.7269\n",
      "Epoch 4/140\n",
      "600/600 [==============================] - 400s 667ms/step - loss: 0.5744 - acc: 0.6960 - val_loss: 0.5254 - val_acc: 0.7281\n",
      "Epoch 5/140\n",
      "600/600 [==============================] - 377s 628ms/step - loss: 0.5571 - acc: 0.7039 - val_loss: 0.4679 - val_acc: 0.7725\n",
      "Epoch 6/140\n",
      "600/600 [==============================] - 397s 662ms/step - loss: 0.5535 - acc: 0.7073 - val_loss: 0.4865 - val_acc: 0.7531\n",
      "Epoch 7/140\n",
      "600/600 [==============================] - 393s 654ms/step - loss: 0.5414 - acc: 0.7184 - val_loss: 0.4531 - val_acc: 0.7801\n",
      "Epoch 8/140\n",
      "600/600 [==============================] - 394s 657ms/step - loss: 0.5330 - acc: 0.7244 - val_loss: 0.4709 - val_acc: 0.7649\n",
      "Epoch 9/140\n",
      "600/600 [==============================] - 390s 650ms/step - loss: 0.5260 - acc: 0.7307 - val_loss: 0.4811 - val_acc: 0.7525\n",
      "Epoch 10/140\n",
      "600/600 [==============================] - 376s 627ms/step - loss: 0.5141 - acc: 0.7380 - val_loss: 0.4378 - val_acc: 0.7795\n",
      "Epoch 11/140\n",
      "600/600 [==============================] - 378s 630ms/step - loss: 0.5031 - acc: 0.7463 - val_loss: 0.4229 - val_acc: 0.7959\n",
      "Epoch 12/140\n",
      "600/600 [==============================] - 376s 627ms/step - loss: 0.4953 - acc: 0.7541 - val_loss: 0.4438 - val_acc: 0.7919\n",
      "Epoch 13/140\n",
      "600/600 [==============================] - 376s 626ms/step - loss: 0.4902 - acc: 0.7528 - val_loss: 0.4010 - val_acc: 0.8054\n",
      "Epoch 14/140\n",
      "600/600 [==============================] - 396s 661ms/step - loss: 0.4833 - acc: 0.7601 - val_loss: 0.4843 - val_acc: 0.7529\n",
      "Epoch 15/140\n",
      "600/600 [==============================] - 399s 665ms/step - loss: 0.4679 - acc: 0.7681 - val_loss: 0.4214 - val_acc: 0.8044\n",
      "Epoch 16/140\n",
      "600/600 [==============================] - 381s 635ms/step - loss: 0.4613 - acc: 0.7709 - val_loss: 0.4488 - val_acc: 0.7763\n",
      "Epoch 17/140\n",
      "600/600 [==============================] - 390s 650ms/step - loss: 0.4573 - acc: 0.7772 - val_loss: 0.4010 - val_acc: 0.8112\n",
      "Epoch 18/140\n",
      "600/600 [==============================] - 390s 650ms/step - loss: 0.4494 - acc: 0.7841 - val_loss: 0.4181 - val_acc: 0.7987\n",
      "Epoch 19/140\n",
      "600/600 [==============================] - 391s 651ms/step - loss: 0.4417 - acc: 0.7857 - val_loss: 0.4128 - val_acc: 0.8010\n",
      "Epoch 20/140\n",
      "600/600 [==============================] - 380s 634ms/step - loss: 0.4361 - acc: 0.7916 - val_loss: 0.3781 - val_acc: 0.8302\n",
      "Epoch 21/140\n",
      "600/600 [==============================] - 357s 595ms/step - loss: 0.4284 - acc: 0.7982 - val_loss: 0.3689 - val_acc: 0.8344\n",
      "Epoch 22/140\n",
      "600/600 [==============================] - 350s 584ms/step - loss: 0.4217 - acc: 0.7983 - val_loss: 0.3806 - val_acc: 0.8289\n",
      "Epoch 23/140\n",
      "600/600 [==============================] - 355s 592ms/step - loss: 0.4176 - acc: 0.8034 - val_loss: 0.3936 - val_acc: 0.8162\n",
      "Epoch 24/140\n",
      "600/600 [==============================] - 351s 586ms/step - loss: 0.4085 - acc: 0.8074 - val_loss: 0.3459 - val_acc: 0.8387\n",
      "Epoch 25/140\n",
      "600/600 [==============================] - 349s 581ms/step - loss: 0.4070 - acc: 0.8065 - val_loss: 0.4353 - val_acc: 0.8099\n",
      "Epoch 26/140\n",
      "600/600 [==============================] - 352s 587ms/step - loss: 0.4008 - acc: 0.8142 - val_loss: 0.3247 - val_acc: 0.8599\n",
      "Epoch 27/140\n",
      "600/600 [==============================] - 356s 594ms/step - loss: 0.4003 - acc: 0.8132 - val_loss: 0.4996 - val_acc: 0.7575\n",
      "Epoch 28/140\n",
      "600/600 [==============================] - 357s 595ms/step - loss: 0.3922 - acc: 0.8193 - val_loss: 0.4825 - val_acc: 0.7814\n",
      "Epoch 29/140\n",
      "600/600 [==============================] - 372s 620ms/step - loss: 0.3866 - acc: 0.8217 - val_loss: 0.3419 - val_acc: 0.8447\n",
      "Epoch 30/140\n",
      "600/600 [==============================] - 424s 707ms/step - loss: 0.3873 - acc: 0.8208 - val_loss: 0.3191 - val_acc: 0.8538\n",
      "Epoch 31/140\n",
      "600/600 [==============================] - 410s 684ms/step - loss: 0.3850 - acc: 0.8214 - val_loss: 0.3263 - val_acc: 0.8568\n",
      "Epoch 32/140\n",
      "600/600 [==============================] - 416s 693ms/step - loss: 0.3778 - acc: 0.8255 - val_loss: 0.3636 - val_acc: 0.8365\n",
      "Epoch 33/140\n",
      "600/600 [==============================] - 408s 681ms/step - loss: 0.3800 - acc: 0.8256 - val_loss: 0.3168 - val_acc: 0.8694\n",
      "Epoch 34/140\n",
      "600/600 [==============================] - 423s 705ms/step - loss: 0.3759 - acc: 0.8298 - val_loss: 0.3286 - val_acc: 0.8580\n",
      "Epoch 35/140\n",
      "600/600 [==============================] - 415s 691ms/step - loss: 0.3704 - acc: 0.8305 - val_loss: 0.3376 - val_acc: 0.8466\n",
      "Epoch 36/140\n",
      "600/600 [==============================] - 421s 702ms/step - loss: 0.3753 - acc: 0.8304 - val_loss: 0.3052 - val_acc: 0.8600\n",
      "Epoch 37/140\n",
      "600/600 [==============================] - 420s 699ms/step - loss: 0.3696 - acc: 0.8310 - val_loss: 0.3336 - val_acc: 0.8612\n",
      "Epoch 38/140\n",
      "600/600 [==============================] - 424s 706ms/step - loss: 0.3674 - acc: 0.8321 - val_loss: 0.3485 - val_acc: 0.8441\n",
      "Epoch 39/140\n",
      "600/600 [==============================] - 421s 702ms/step - loss: 0.3678 - acc: 0.8336 - val_loss: 0.3547 - val_acc: 0.8369\n",
      "Epoch 40/140\n",
      "600/600 [==============================] - 376s 626ms/step - loss: 0.3632 - acc: 0.8322 - val_loss: 0.3043 - val_acc: 0.8682\n",
      "Epoch 41/140\n",
      "600/600 [==============================] - 368s 613ms/step - loss: 0.3647 - acc: 0.8316 - val_loss: 0.2991 - val_acc: 0.8638\n",
      "Epoch 42/140\n",
      "600/600 [==============================] - 368s 613ms/step - loss: 0.3574 - acc: 0.8364 - val_loss: 0.4001 - val_acc: 0.8352\n",
      "Epoch 43/140\n",
      "600/600 [==============================] - 353s 588ms/step - loss: 0.3633 - acc: 0.8375 - val_loss: 0.3253 - val_acc: 0.8542\n",
      "Epoch 44/140\n",
      "600/600 [==============================] - 350s 584ms/step - loss: 0.3554 - acc: 0.8405 - val_loss: 0.3029 - val_acc: 0.8719\n",
      "Epoch 45/140\n",
      "600/600 [==============================] - 375s 624ms/step - loss: 0.3565 - acc: 0.8372 - val_loss: 0.3443 - val_acc: 0.8441\n",
      "Epoch 46/140\n",
      "600/600 [==============================] - 379s 632ms/step - loss: 0.3577 - acc: 0.8369 - val_loss: 0.2947 - val_acc: 0.8777\n",
      "Epoch 47/140\n",
      "600/600 [==============================] - 403s 672ms/step - loss: 0.3551 - acc: 0.8394 - val_loss: 0.3289 - val_acc: 0.8544\n",
      "Epoch 48/140\n",
      "600/600 [==============================] - 420s 701ms/step - loss: 0.3553 - acc: 0.8388 - val_loss: 0.4512 - val_acc: 0.7890\n",
      "Epoch 49/140\n",
      "600/600 [==============================] - 422s 704ms/step - loss: 0.3577 - acc: 0.8397 - val_loss: 0.2889 - val_acc: 0.8802\n",
      "Epoch 50/140\n",
      "600/600 [==============================] - 411s 685ms/step - loss: 0.3451 - acc: 0.8454 - val_loss: 0.2909 - val_acc: 0.8762\n",
      "Epoch 51/140\n",
      "600/600 [==============================] - 421s 702ms/step - loss: 0.3447 - acc: 0.8448 - val_loss: 0.3266 - val_acc: 0.8739\n",
      "Epoch 52/140\n",
      "600/600 [==============================] - 412s 687ms/step - loss: 0.3478 - acc: 0.8399 - val_loss: 0.2991 - val_acc: 0.8669\n",
      "Epoch 53/140\n",
      "600/600 [==============================] - 409s 681ms/step - loss: 0.3455 - acc: 0.8456 - val_loss: 0.2787 - val_acc: 0.8756\n",
      "Epoch 54/140\n",
      "600/600 [==============================] - 421s 702ms/step - loss: 0.3492 - acc: 0.8438 - val_loss: 0.2923 - val_acc: 0.8840\n",
      "Epoch 55/140\n",
      "600/600 [==============================] - 427s 712ms/step - loss: 0.3540 - acc: 0.8428 - val_loss: 0.3305 - val_acc: 0.8619\n",
      "Epoch 56/140\n",
      "600/600 [==============================] - 432s 720ms/step - loss: 0.3462 - acc: 0.8470 - val_loss: 0.2858 - val_acc: 0.8762\n",
      "Epoch 57/140\n",
      "600/600 [==============================] - 432s 720ms/step - loss: 0.3518 - acc: 0.8424 - val_loss: 0.3003 - val_acc: 0.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/140\n",
      "600/600 [==============================] - 433s 722ms/step - loss: 0.3398 - acc: 0.8510 - val_loss: 0.2946 - val_acc: 0.8720\n",
      "Epoch 59/140\n",
      "600/600 [==============================] - 389s 648ms/step - loss: 0.3465 - acc: 0.8464 - val_loss: 0.3199 - val_acc: 0.8625\n",
      "Epoch 60/140\n",
      "600/600 [==============================] - 409s 682ms/step - loss: 0.3509 - acc: 0.8454 - val_loss: 0.3029 - val_acc: 0.8644\n",
      "Epoch 61/140\n",
      "600/600 [==============================] - 439s 731ms/step - loss: 0.3480 - acc: 0.8465 - val_loss: 0.3744 - val_acc: 0.8403\n",
      "Epoch 62/140\n",
      "600/600 [==============================] - 393s 655ms/step - loss: 0.3464 - acc: 0.8488 - val_loss: 0.3054 - val_acc: 0.8681\n",
      "Epoch 63/140\n",
      "600/600 [==============================] - 413s 688ms/step - loss: 0.3458 - acc: 0.8432 - val_loss: 0.3170 - val_acc: 0.8644\n",
      "Epoch 64/140\n",
      "600/600 [==============================] - 413s 688ms/step - loss: 0.3421 - acc: 0.8501 - val_loss: 0.3135 - val_acc: 0.8657\n",
      "Epoch 65/140\n",
      "600/600 [==============================] - 408s 681ms/step - loss: 0.3481 - acc: 0.8483 - val_loss: 0.3209 - val_acc: 0.8681\n",
      "Epoch 66/140\n",
      "600/600 [==============================] - 409s 682ms/step - loss: 0.3473 - acc: 0.8447 - val_loss: 0.2494 - val_acc: 0.8961\n",
      "Epoch 67/140\n",
      "600/600 [==============================] - 407s 679ms/step - loss: 0.3475 - acc: 0.8465 - val_loss: 0.3059 - val_acc: 0.8771\n",
      "Epoch 68/140\n",
      "600/600 [==============================] - 402s 670ms/step - loss: 0.3383 - acc: 0.8548 - val_loss: 0.2920 - val_acc: 0.8744\n",
      "Epoch 69/140\n",
      "600/600 [==============================] - 414s 690ms/step - loss: 0.3430 - acc: 0.8475 - val_loss: 0.3159 - val_acc: 0.8669\n",
      "Epoch 70/140\n",
      "600/600 [==============================] - 409s 681ms/step - loss: 0.3429 - acc: 0.8483 - val_loss: 0.3296 - val_acc: 0.8619\n",
      "Epoch 71/140\n",
      "600/600 [==============================] - 414s 691ms/step - loss: 0.3339 - acc: 0.8547 - val_loss: 0.3700 - val_acc: 0.8581\n",
      "Epoch 72/140\n",
      "600/600 [==============================] - 405s 675ms/step - loss: 0.3478 - acc: 0.8483 - val_loss: 0.3200 - val_acc: 0.8688\n",
      "Epoch 73/140\n",
      "600/600 [==============================] - 408s 680ms/step - loss: 0.3416 - acc: 0.8500 - val_loss: 0.3008 - val_acc: 0.8796\n",
      "Epoch 74/140\n",
      "600/600 [==============================] - 424s 707ms/step - loss: 0.3482 - acc: 0.8453 - val_loss: 0.3076 - val_acc: 0.8719\n",
      "Epoch 75/140\n",
      "600/600 [==============================] - 384s 639ms/step - loss: 0.3373 - acc: 0.8504 - val_loss: 0.4075 - val_acc: 0.8245\n",
      "Epoch 76/140\n",
      "600/600 [==============================] - 410s 684ms/step - loss: 0.3430 - acc: 0.8489 - val_loss: 0.3298 - val_acc: 0.8536\n",
      "Epoch 77/140\n",
      "600/600 [==============================] - 414s 690ms/step - loss: 0.3389 - acc: 0.8493 - val_loss: 0.2951 - val_acc: 0.8750\n",
      "Epoch 78/140\n",
      "600/600 [==============================] - 408s 681ms/step - loss: 0.3425 - acc: 0.8513 - val_loss: 0.2952 - val_acc: 0.8821\n",
      "Epoch 79/140\n",
      "600/600 [==============================] - 346s 577ms/step - loss: 0.3464 - acc: 0.8486 - val_loss: 0.3152 - val_acc: 0.8638\n",
      "Epoch 80/140\n",
      "600/600 [==============================] - 329s 548ms/step - loss: 0.3394 - acc: 0.8487 - val_loss: 0.2820 - val_acc: 0.8891\n",
      "Epoch 81/140\n",
      "600/600 [==============================] - 346s 577ms/step - loss: 0.3482 - acc: 0.8513 - val_loss: 0.3112 - val_acc: 0.8676\n",
      "Epoch 82/140\n",
      "600/600 [==============================] - 342s 570ms/step - loss: 0.3411 - acc: 0.8497 - val_loss: 0.2973 - val_acc: 0.8762\n",
      "Epoch 83/140\n",
      "600/600 [==============================] - 336s 560ms/step - loss: 0.3411 - acc: 0.8515 - val_loss: 0.3455 - val_acc: 0.8644\n",
      "Epoch 84/140\n",
      "600/600 [==============================] - 335s 558ms/step - loss: 0.3441 - acc: 0.8454 - val_loss: 0.2948 - val_acc: 0.8783\n",
      "Epoch 85/140\n",
      "600/600 [==============================] - 331s 551ms/step - loss: 0.3393 - acc: 0.8508 - val_loss: 0.3002 - val_acc: 0.8725\n",
      "Epoch 86/140\n",
      "600/600 [==============================] - 336s 560ms/step - loss: 0.3475 - acc: 0.8489 - val_loss: 0.3641 - val_acc: 0.8365\n",
      "Epoch 87/140\n",
      "600/600 [==============================] - 324s 539ms/step - loss: 0.3381 - acc: 0.8533 - val_loss: 0.2928 - val_acc: 0.8701\n",
      "Epoch 88/140\n",
      "600/600 [==============================] - 340s 567ms/step - loss: 0.3345 - acc: 0.8549 - val_loss: 0.3610 - val_acc: 0.8494\n",
      "Epoch 89/140\n",
      "600/600 [==============================] - 344s 574ms/step - loss: 0.3484 - acc: 0.8501 - val_loss: 0.2631 - val_acc: 0.8815\n",
      "Epoch 90/140\n",
      "600/600 [==============================] - 343s 572ms/step - loss: 0.3447 - acc: 0.8509 - val_loss: 0.3733 - val_acc: 0.8511\n",
      "Epoch 91/140\n",
      "600/600 [==============================] - 339s 565ms/step - loss: 0.3419 - acc: 0.8527 - val_loss: 0.3036 - val_acc: 0.8706\n",
      "Epoch 92/140\n",
      "600/600 [==============================] - 315s 526ms/step - loss: 0.3429 - acc: 0.8497 - val_loss: 0.2892 - val_acc: 0.8796\n",
      "Epoch 93/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3435 - acc: 0.8455 - val_loss: 0.3331 - val_acc: 0.8549\n",
      "Epoch 94/140\n",
      "600/600 [==============================] - 318s 530ms/step - loss: 0.3396 - acc: 0.8505 - val_loss: 0.2856 - val_acc: 0.8875\n",
      "Epoch 95/140\n",
      "600/600 [==============================] - 315s 525ms/step - loss: 0.3417 - acc: 0.8543 - val_loss: 0.3115 - val_acc: 0.8612\n",
      "Epoch 96/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3348 - acc: 0.8520 - val_loss: 0.3070 - val_acc: 0.8790\n",
      "Epoch 97/140\n",
      "600/600 [==============================] - 318s 530ms/step - loss: 0.3393 - acc: 0.8518 - val_loss: 0.2969 - val_acc: 0.8788\n",
      "Epoch 98/140\n",
      "600/600 [==============================] - 319s 531ms/step - loss: 0.3370 - acc: 0.8516 - val_loss: 0.2982 - val_acc: 0.8809\n",
      "Epoch 99/140\n",
      "600/600 [==============================] - 318s 530ms/step - loss: 0.3499 - acc: 0.8497 - val_loss: 0.2768 - val_acc: 0.8764\n",
      "Epoch 100/140\n",
      "600/600 [==============================] - 315s 524ms/step - loss: 0.3395 - acc: 0.8512 - val_loss: 0.2999 - val_acc: 0.8881\n",
      "Epoch 101/140\n",
      "600/600 [==============================] - 315s 526ms/step - loss: 0.3437 - acc: 0.8519 - val_loss: 0.3061 - val_acc: 0.8619\n",
      "Epoch 102/140\n",
      "600/600 [==============================] - 320s 533ms/step - loss: 0.3387 - acc: 0.8494 - val_loss: 0.3364 - val_acc: 0.8771\n",
      "Epoch 103/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3426 - acc: 0.8507 - val_loss: 0.2829 - val_acc: 0.8800\n",
      "Epoch 104/140\n",
      "600/600 [==============================] - 313s 522ms/step - loss: 0.3415 - acc: 0.8493 - val_loss: 0.3180 - val_acc: 0.8752\n",
      "Epoch 105/140\n",
      "600/600 [==============================] - 320s 533ms/step - loss: 0.3353 - acc: 0.8531 - val_loss: 0.2831 - val_acc: 0.8840\n",
      "Epoch 106/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3366 - acc: 0.8530 - val_loss: 0.2810 - val_acc: 0.8900\n",
      "Epoch 107/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3490 - acc: 0.8484 - val_loss: 0.3071 - val_acc: 0.8733\n",
      "Epoch 108/140\n",
      "600/600 [==============================] - 321s 534ms/step - loss: 0.3365 - acc: 0.8581 - val_loss: 0.3161 - val_acc: 0.8796\n",
      "Epoch 109/140\n",
      "600/600 [==============================] - 315s 525ms/step - loss: 0.3372 - acc: 0.8497 - val_loss: 0.3245 - val_acc: 0.8662\n",
      "Epoch 110/140\n",
      "600/600 [==============================] - 314s 523ms/step - loss: 0.3355 - acc: 0.8533 - val_loss: 0.3415 - val_acc: 0.8473\n",
      "Epoch 111/140\n",
      "600/600 [==============================] - 318s 529ms/step - loss: 0.3461 - acc: 0.8477 - val_loss: 0.2771 - val_acc: 0.8904\n",
      "Epoch 112/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3446 - acc: 0.8476 - val_loss: 0.3003 - val_acc: 0.8800\n",
      "Epoch 113/140\n",
      "600/600 [==============================] - 320s 533ms/step - loss: 0.3462 - acc: 0.8498 - val_loss: 0.2895 - val_acc: 0.8790\n",
      "Epoch 114/140\n",
      "600/600 [==============================] - 316s 527ms/step - loss: 0.3486 - acc: 0.8477 - val_loss: 0.3266 - val_acc: 0.8631\n",
      "Epoch 115/140\n",
      "600/600 [==============================] - 316s 526ms/step - loss: 0.3454 - acc: 0.8491 - val_loss: 0.6372 - val_acc: 0.8231\n",
      "Epoch 116/140\n",
      "600/600 [==============================] - 314s 523ms/step - loss: 0.3473 - acc: 0.8468 - val_loss: 0.3347 - val_acc: 0.8682\n",
      "Epoch 117/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 317s 529ms/step - loss: 0.3464 - acc: 0.8488 - val_loss: 0.3112 - val_acc: 0.8644\n",
      "Epoch 118/140\n",
      "600/600 [==============================] - 318s 530ms/step - loss: 0.3494 - acc: 0.8457 - val_loss: 0.3294 - val_acc: 0.8530\n",
      "Epoch 119/140\n",
      "600/600 [==============================] - 329s 548ms/step - loss: 0.3514 - acc: 0.8459 - val_loss: 0.3323 - val_acc: 0.8599\n",
      "Epoch 120/140\n",
      "600/600 [==============================] - 317s 529ms/step - loss: 0.3548 - acc: 0.8480 - val_loss: 0.3105 - val_acc: 0.8719\n",
      "Epoch 121/140\n",
      "600/600 [==============================] - 313s 522ms/step - loss: 0.3519 - acc: 0.8471 - val_loss: 0.3936 - val_acc: 0.8638\n",
      "Epoch 122/140\n",
      "600/600 [==============================] - 318s 531ms/step - loss: 0.3458 - acc: 0.8471 - val_loss: 0.4397 - val_acc: 0.8131\n",
      "Epoch 123/140\n",
      "600/600 [==============================] - 320s 534ms/step - loss: 0.3491 - acc: 0.8478 - val_loss: 0.3400 - val_acc: 0.8588\n",
      "Epoch 124/140\n",
      "600/600 [==============================] - 312s 520ms/step - loss: 0.3413 - acc: 0.8497 - val_loss: 0.3149 - val_acc: 0.8745\n",
      "Epoch 125/140\n",
      "600/600 [==============================] - 318s 531ms/step - loss: 0.3473 - acc: 0.8470 - val_loss: 0.2822 - val_acc: 0.8872\n",
      "Epoch 126/140\n",
      "600/600 [==============================] - 320s 533ms/step - loss: 0.3465 - acc: 0.8474 - val_loss: 0.2989 - val_acc: 0.8706\n",
      "Epoch 127/140\n",
      "600/600 [==============================] - 316s 526ms/step - loss: 0.3557 - acc: 0.8481 - val_loss: 0.2867 - val_acc: 0.8916\n",
      "Epoch 128/140\n",
      "600/600 [==============================] - 313s 521ms/step - loss: 0.3467 - acc: 0.8485 - val_loss: 0.3212 - val_acc: 0.8650\n",
      "Epoch 129/140\n",
      "600/600 [==============================] - 319s 531ms/step - loss: 0.3533 - acc: 0.8489 - val_loss: 0.4062 - val_acc: 0.8344\n",
      "Epoch 130/140\n",
      "600/600 [==============================] - 314s 524ms/step - loss: 0.3424 - acc: 0.8506 - val_loss: 0.3076 - val_acc: 0.8733\n",
      "Epoch 131/140\n",
      "600/600 [==============================] - 318s 530ms/step - loss: 0.3486 - acc: 0.8467 - val_loss: 0.3082 - val_acc: 0.8790\n",
      "Epoch 132/140\n",
      "600/600 [==============================] - 318s 530ms/step - loss: 0.3443 - acc: 0.8507 - val_loss: 0.2938 - val_acc: 0.8719\n",
      "Epoch 133/140\n",
      "600/600 [==============================] - 313s 522ms/step - loss: 0.3559 - acc: 0.8478 - val_loss: 0.2913 - val_acc: 0.8783\n",
      "Epoch 134/140\n",
      "600/600 [==============================] - 319s 531ms/step - loss: 0.3552 - acc: 0.8447 - val_loss: 0.3015 - val_acc: 0.8764\n",
      "Epoch 135/140\n",
      "600/600 [==============================] - 315s 525ms/step - loss: 0.3443 - acc: 0.8503 - val_loss: 0.3172 - val_acc: 0.8644\n",
      "Epoch 136/140\n",
      "600/600 [==============================] - 313s 522ms/step - loss: 0.3548 - acc: 0.8440 - val_loss: 0.6712 - val_acc: 0.6610\n",
      "Epoch 137/140\n",
      "600/600 [==============================] - 318s 529ms/step - loss: 0.3684 - acc: 0.8377 - val_loss: 0.3277 - val_acc: 0.8821\n",
      "Epoch 138/140\n",
      "600/600 [==============================] - 315s 525ms/step - loss: 0.3551 - acc: 0.8473 - val_loss: 0.3282 - val_acc: 0.8694\n",
      "Epoch 139/140\n",
      "600/600 [==============================] - 320s 533ms/step - loss: 0.3571 - acc: 0.8435 - val_loss: 0.3016 - val_acc: 0.8802\n",
      "Epoch 140/140\n",
      "600/600 [==============================] - 317s 529ms/step - loss: 0.3502 - acc: 0.8481 - val_loss: 0.3039 - val_acc: 0.8644\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=-.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dir = r'C:\\sieci_neuronowe\\Training Official v2'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=32,\n",
    "            class_mode='binary')\n",
    "\n",
    "validation_dir = r'C:\\sieci_neuronowe\\Validation Official v2'\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "            validation_dir,\n",
    "            target_size=(150, 150),\n",
    "            batch_size=32,\n",
    "            class_mode='binary')\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=600,\n",
    "        epochs=140,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=50)\n",
    "\n",
    "#100 epochs git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generowanie wykresów\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs,acc,'bo', label='Skuteczność trenowania')\n",
    "plt.plot(epochs,val_acc,'b', label='Skuteczność walidacji')\n",
    "plt.title('Skuteczność trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,loss,'bo', label='Strata trenowania')\n",
    "plt.plot(epochs,val_loss,'b', label='Strata walidacji')\n",
    "plt.title('Strata trenowania i walidacji')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Rozpoznawanie_wieku_i_plci.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
